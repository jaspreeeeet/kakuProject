#!/usr/bin/env python3
"""
Install and Test BLIP Model on E: Drive
BLIP (Bootstrapping Language-Image Pre-training) for image captioning
"""

import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import os
import gc
import glob

print("\n" + "="*70)
print("ğŸš€ INSTALLING & TESTING BLIP MODEL ON E: DRIVE")
print("="*70 + "\n")

# Set cache to E: drive
CACHE_DIR = r"E:\Rajeev\esp 32\esp 32\.cache\huggingface"
os.environ['HUGGINGFACE_HUB_CACHE'] = CACHE_DIR
os.environ['TRANSFORMERS_CACHE'] = CACHE_DIR
os.makedirs(CACHE_DIR, exist_ok=True)

print(f"ğŸ“ Cache Directory: {CACHE_DIR}")
print(f"ğŸ’¾ Storage: E: drive")

# Use smaller BLIP variant
MODEL_ID = "Salesforce/blip-image-captioning-base"

print(f"\nğŸ¯ Model: {MODEL_ID}")
print(f"ğŸ“Š Size: ~350 MB")
print("\nâ³ Downloading & Installing BLIP model...\n")

try:
    # Download processor
    print("ğŸ“¥ Downloading processor...")
    processor = BlipProcessor.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)
    print("âœ… Processor loaded successfully")
    
    # Download model
    print("ğŸ“¥ Downloading BLIP model weights (~350MB)...")
    print("â³ This may take 2-5 minutes on first run...\n")
    
    model = BlipForConditionalGeneration.from_pretrained(
        MODEL_ID,
        cache_dir=CACHE_DIR,
        torch_dtype=torch.float32,
        device_map=None
    )
    print("\nâœ… BLIP model loaded successfully!")
    
    # Move to device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    print(f"âœ… Running on: {device.upper()}")
    
    # Find test images
    print("\nğŸ“¸ Searching for test images...")
    image_patterns = [
        "uploads/images/*.jpg",
        "uploads/images/*.jpeg",
        "uploads/images/*.png",
    ]
    
    found_images = []
    for pattern in image_patterns:
        found_images.extend(glob.glob(pattern))
    
    if not found_images:
        print("âŒ No images found in uploads/images/")
    else:
        print(f"âœ… Found {len(found_images)} images\n")
        
        print("="*70)
        print("ğŸ¤– TESTING BLIP MODEL ON IMAGES")
        print("="*70 + "\n")
        
        for idx, img_path in enumerate(found_images[:3], 1):
            print(f"\n{'â”€'*70}")
            print(f"ğŸ“· Image {idx}: {os.path.basename(img_path)}")
            print(f"{'â”€'*70}")
            
            try:
                # Load image
                image = Image.open(img_path).convert("RGB")
                print(f"ğŸ“ Resolution: {image.size[0]}Ã—{image.size[1]}")
                
                # Process and generate caption
                print("ğŸ¤– Generating caption...")
                inputs = processor(image, return_tensors="pt").to(device)
                
                with torch.no_grad():
                    out = model.generate(**inputs, max_length=50)
                
                caption = processor.decode(out[0], skip_special_tokens=True)
                
                print(f"âœ… Caption generated!")
                print(f"ğŸ¯ BLIP Caption: {caption}\n")
                
            except Exception as e:
                print(f"âŒ Error: {e}\n")
        
        print("="*70)
        print("âœ… BLIP MODEL TEST COMPLETED SUCCESSFULLY!")
        print("="*70)
        print(f"""
ğŸ“Š BLIP Model Installed Successfully
   â””â”€ Location: {CACHE_DIR}
   â””â”€ Model: Salesforce/blip-image-captioning-base
   â””â”€ Size: ~350 MB
   â””â”€ Type: Image Captioning (Excellent quality captions)

ğŸ¯ Next Steps:
   1. To use BLIP in your app.py, create a new endpoint
   2. Or update app.py to support multiple models
   3. BLIP generates more detailed captions than Google ViT

ğŸ’¡ Performance:
   - BLIP: ~2-5 seconds per image (CPU)
   - Google ViT: ~0.5 seconds per image (faster)
   - Use BLIP for high-quality captions, ViT for speed

âœ¨ Model is now cached on E: drive for future use!
        """)
        
except Exception as e:
    print(f"\nâŒ Error during installation: {e}")
    import traceback
    traceback.print_exc()

# Cleanup
gc.collect()
print("\nğŸ§¹ Cleanup complete!")
